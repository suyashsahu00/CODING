{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd5622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a dummy male voice audio file at dummy_male_voice.wav for demonstration.\n",
      "Creating a dummy female voice audio file at dummy_female_voice.wav for demonstration.\n",
      "\n",
      "Extracting features from training data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "No librosa attribute soundfile",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExtracting features from training data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, audio_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_audio_paths):\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     features = \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m         X.append(features)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mextract_features\u001b[39m\u001b[34m(audio_path, sr)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Extract Pitch (F0)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Using 'pyin' for more robust pitch tracking\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# fmin and fmax define the range of frequencies to search for F0\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m f0, voiced_flag, voiced_probs = \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoundfile\u001b[49m.pyin(y, fmin=librosa.note_to_hz(\u001b[33m'\u001b[39m\u001b[33mC2\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     30\u001b[39m                                                     fmax=librosa.note_to_hz(\u001b[33m'\u001b[39m\u001b[33mC5\u001b[39m\u001b[33m'\u001b[39m), sr=sr)\n\u001b[32m     31\u001b[39m f0_mean = np.nanmean(f0) \u001b[38;5;28;01mif\u001b[39;00m np.any(~np.isnan(f0)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;66;03m# Handle cases with no detected pitch\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Extract MFCCs\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# n_mfcc=13 is a common choice for speech\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\suyas\\OneDrive\\Desktop\\CODING\\CODING\\.venv\\Lib\\site-packages\\lazy_loader\\__init__.py:94\u001b[39m, in \u001b[36mattach.<locals>.__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attr\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: No librosa attribute soundfile"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "import soundfile as sf # For potentially handling different audio formats\n",
    "\n",
    "# --- 1. Feature Extraction Function ---\n",
    "def extract_features(audio_path, sr=22050):\n",
    "    \"\"\"\n",
    "    Extracts pitch (F0) and Mel-Frequency Cepstral Coefficients (MFCCs) from an audio file.\n",
    "    Handles potential FileNotFoundError.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=sr)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Audio file not found at '{audio_path}'. Skipping.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file '{audio_path}': {e}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Extract Pitch (F0)\n",
    "    # Using 'pyin' for more robust pitch tracking\n",
    "    # fmin and fmax define the range of frequencies to search for F0\n",
    "    f0, voiced_flag, voiced_probs = librosa.soundfile.pyin(y, fmin=librosa.note_to_hz('C2'),\n",
    "                                                        fmax=librosa.note_to_hz('C5'), sr=sr)\n",
    "    f0_mean = np.nanmean(f0) if np.any(~np.isnan(f0)) else 0 # Handle cases with no detected pitch\n",
    "\n",
    "    # Extract MFCCs\n",
    "    # n_mfcc=13 is a common choice for speech\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "\n",
    "    # Combine features\n",
    "    features = np.concatenate(([f0_mean], mfccs_mean))\n",
    "    return features\n",
    "\n",
    "# --- 2. Dummy Dataset Creation (REPLACE THIS WITH YOUR REAL, LABELED DATA) ---\n",
    "# For demonstration purposes, we'll create a very small, illustrative dummy dataset.\n",
    "# In a real-world scenario, you would have many audio files for each gender.\n",
    "# You need to replace these paths with actual paths to YOUR labeled male/female audio files.\n",
    "\n",
    "# Create some dummy audio files for demonstration if they don't exist\n",
    "# In a real scenario, you'd load your actual dataset\n",
    "dummy_male_path = \"dummy_male_voice.wav\"\n",
    "dummy_female_path = \"dummy_female_voice.wav\"\n",
    "\n",
    "if not os.path.exists(dummy_male_path):\n",
    "    print(f\"Creating a dummy male voice audio file at {dummy_male_path} for demonstration.\")\n",
    "    # Simulate a male voice (lower frequency sine wave)\n",
    "    sr_dummy = 22050\n",
    "    duration_dummy = 2 # seconds\n",
    "    t = np.linspace(0, duration_dummy, int(sr_dummy * duration_dummy), endpoint=False)\n",
    "    male_freq = 120 # Hz (typical male pitch range)\n",
    "    dummy_male_audio = 0.5 * np.sin(2 * np.pi * male_freq * t)\n",
    "    sf.write(dummy_male_path, dummy_male_audio, sr_dummy)\n",
    "\n",
    "if not os.path.exists(dummy_female_path):\n",
    "    print(f\"Creating a dummy female voice audio file at {dummy_female_path} for demonstration.\")\n",
    "    # Simulate a female voice (higher frequency sine wave)\n",
    "    sr_dummy = 22050\n",
    "    duration_dummy = 2 # seconds\n",
    "    t = np.linspace(0, duration_dummy, int(sr_dummy * duration_dummy), endpoint=False)\n",
    "    female_freq = 220 # Hz (typical female pitch range)\n",
    "    dummy_female_audio = 0.5 * np.sin(2 * np.pi * female_freq * t)\n",
    "    sf.write(dummy_female_path, dummy_female_audio, sr_dummy)\n",
    "\n",
    "# Define the training data paths and labels\n",
    "# IMPORTANT: Replace these with paths to your actual, properly labeled male and female audio files.\n",
    "train_audio_paths = [\n",
    "    dummy_male_path,\n",
    "    dummy_male_path,\n",
    "    dummy_male_path,\n",
    "    dummy_female_path,\n",
    "    dummy_female_path,\n",
    "    dummy_female_path,\n",
    "]\n",
    "\n",
    "train_labels = [0, 0, 0, 1, 1, 1] # 0 for male, 1 for female\n",
    "\n",
    "# --- Process the training data ---\n",
    "X = []\n",
    "y = []\n",
    "print(\"\\nExtracting features from training data...\")\n",
    "for i, audio_path in enumerate(train_audio_paths):\n",
    "    features = extract_features(audio_path)\n",
    "    if features is not None:\n",
    "        X.append(features)\n",
    "        y.append(train_labels[i])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "if len(X) == 0:\n",
    "    print(\"No features extracted from the training dataset. Cannot train model. Please check audio paths.\")\n",
    "else:\n",
    "    # --- 3. Train a Machine Learning Model ---\n",
    "    # Split data into training and testing sets\n",
    "    # test_size=0.3 means 30% of data will be used for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Initialize and train a Logistic Regression model\n",
    "    # Logistic Regression is a simple, yet often effective, baseline classifier\n",
    "    model = LogisticRegression(max_iter=1000, solver='lbfgs') # Increased max_iter and specified solver for convergence\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\n--- Model Evaluation (on dummy data) ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Male', 'Female']))\n",
    "    print(\"Note: Accuracy on dummy data is not indicative of real-world performance.\")\n",
    "\n",
    "    # --- 4. Predict on Your Uploaded Audio File ---\n",
    "    your_audio_file = \"Video_Created_New_Veo_Launch.wav\" # This is the file you provided\n",
    "\n",
    "    print(f\"\\n--- Predicting gender for your audio file: '{your_audio_file}' ---\")\n",
    "    new_features = extract_features(your_audio_file)\n",
    "\n",
    "    if new_features is not None:\n",
    "        prediction = model.predict(new_features.reshape(1, -1)) # Reshape for single sample prediction\n",
    "        predicted_gender = \"Male\" if prediction[0] == 0 else \"Female\"\n",
    "        print(f\"The predicted gender for '{your_audio_file}' is: {predicted_gender}\")\n",
    "        print(\"\\nDisclaimer: This prediction is based on a very small, dummy training set.\")\n",
    "        print(\"For accurate results, train the model on a large and diverse dataset of male and female voices.\")\n",
    "    else:\n",
    "        print(f\"Could not extract features from '{your_audio_file}'. Please ensure the file is accessible and not corrupted.\")\n",
    "\n",
    "# --- Visualizing the Waveform, Frequency Spectrum, and Mel Spectrogram ---\n",
    "# This part uses the actual 'Video_Created_New_Veo_Launch.wav' for visualization\n",
    "\n",
    "print(f\"\\n--- Visualizing your audio file: '{your_audio_file}' ---\")\n",
    "try:\n",
    "    y_plot, sr_plot = librosa.load(your_audio_file, sr=None)\n",
    "    print(f\"Audio loaded successfully for visualization! Sampling Rate (sr): {sr_plot} Hz\")\n",
    "    print(f\"Audio duration: {len(y_plot)/sr_plot:.2f} seconds\")\n",
    "\n",
    "    # Waveform\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.waveshow(y_plot, sr=sr_plot, color='blue')\n",
    "    plt.title('Audio Waveform')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Frequency Spectrum (FFT)\n",
    "    Y_fft = np.fft.fft(y_plot)\n",
    "    freqs_fft = np.fft.fftfreq(len(Y_fft), 1/sr_plot)\n",
    "    magnitude_fft = np.abs(Y_fft)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(freqs_fft[:len(freqs_fft)//2], magnitude_fft[:len(magnitude_fft)//2])\n",
    "    plt.title(\"Frequency Spectrum (FFT)\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Mel Spectrogram\n",
    "    S_mel = librosa.feature.melspectrogram(y=y_plot, sr=sr_plot)\n",
    "    S_dB_mel = librosa.power_to_db(S_mel, ref=np.max)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(S_dB_mel, sr=sr_plot, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel Spectrogram')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Mel Frequency (Hz)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Your audio file '{your_audio_file}' was not found for visualization.\")\n",
    "    print(\"Please make sure it's in the same directory as the script or provide the full path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during visualization: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
